{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as df\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "df = pd.read_excel(r'/Users/smani/Downloads/skills_taxonomy.xlsx')\n",
    "skills = df[\"skill\"].tolist()\n",
    "skill_l = []\n",
    "for x in skills:\n",
    "    if isinstance(x,str):\n",
    "        for y in x :\n",
    "            if y.isalpha():\n",
    "                skill_l.append(x.lower())\n",
    "                break\n",
    "    else:\n",
    "        pass\n",
    "final_skill = list(dict.fromkeys(skill_l))\n",
    "final_skill = final_skill[:5]\n",
    "base_url  = 'https://en.wikipedia.org/wiki/'\n",
    "nowtime = time.time()\n",
    "f=open(\"WP_scrapnew_\" + str(nowtime) + \".json\", \"w+\")\n",
    "final_data = []\n",
    "for i,s in enumerate(final_skill):\n",
    "    skill_list=[]\n",
    "    url1 = '{}{}'.format(base_url, s)\n",
    "    url2 = '{}{}{}'.format(base_url,s,\"_(disambiguation)\")\n",
    "    call_req= 0\n",
    "    while call_req<5:\n",
    "        try:\n",
    "            resp1 = requests.get(url1, timeout=None, stream=True)\n",
    "        except TimeoutError:\n",
    "            sleep(2)\n",
    "            resp1 = None\n",
    "            continue\n",
    "        call_req = call_req + 1\n",
    "    call_req = 0\n",
    "    while call_req<5:\n",
    "        try:\n",
    "            resp2 = requests.get(url2, timeout=None, stream=True)\n",
    "        except TimeoutError:\n",
    "            sleep(2)\n",
    "            resp2 = None\n",
    "            continue\n",
    "        call_req = call_req + 1\n",
    "    if resp1==None or resp2 ==None:\n",
    "        skill_error_list.append(s)\n",
    "        continue\n",
    "    soup1 = BeautifulSoup(resp1.text, 'lxml')\n",
    "    soup2 = BeautifulSoup(resp2.text, 'lxml')\n",
    "    if resp1.status_code == 200:\n",
    "        for item1 in soup1.select(\"#firstHeading\"):\n",
    "            skill_dict = {'Skill': s, 'URL': url1, 'PageHeading': item1.text}\n",
    "    else:\n",
    "        skill_dict = {'Skill': s, 'URL': \"Web page does not exist\", 'PageHeading': \"NA\"}\n",
    "    if resp2.status_code == 200:\n",
    "        for item2 in soup2.select(\".mw-headline\"):\n",
    "            skill_list.append(item2.text)\n",
    "        # print(skill_list)\n",
    "        skill_dict['Other_use'] = skill_list\n",
    "    else:\n",
    "        skill_dict['Other_use'] = \"NA\"\n",
    "    print(i)\n",
    "    f.write(str(skill_dict) + \",\")\n",
    "    # final_data.append(skill_dict)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as df\n",
    "import pandas as pd\n",
    "import requests\n",
    "#from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "#requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'\n",
    "headers = { 'User-Agent' : user_agent }\n",
    "df = pd.read_excel(r'/Users/smani/Downloads/skills_taxonomy.xlsx')\n",
    "skills = df[\"skill\"].tolist()\n",
    "skill_l = []\n",
    "for x in skills:\n",
    "    if isinstance(x,str):\n",
    "        for y in x :\n",
    "            if y.isalpha():\n",
    "                skill_l.append(x.lower())\n",
    "                break\n",
    "    else:\n",
    "        pass\n",
    "final_skill = list(dict.fromkeys(skill_l))\n",
    "final_skill = final_skill[3398:]\n",
    "base_url  = 'https://en.wikipedia.org/wiki/'\n",
    "nowtime = time.time()\n",
    "f=open(\"WP_scrapnew_\" + str(nowtime) + \".json\", \"w+\")\n",
    "final_data = []\n",
    "\n",
    "for i,s in enumerate(final_skill):\n",
    "    skill_list=[]\n",
    "    url1 = '{}{}'.format(base_url, s)\n",
    "    url2 = '{}{}{}'.format(base_url,s,\"_(disambiguation)\")\n",
    "\n",
    "    #proxy_host = \"proxy.crawlera.com\"\n",
    "    #proxy_port = \"8010\"\n",
    "    #proxy_auth = \"<CRAWLERA API KEY>:\" # Make sure to include ':' at the end\n",
    "    #proxies = {\"https\": \"https://{}@{}:{}/\".format(proxy_auth, proxy_host, proxy_port),\n",
    "      #\"http\": \"http://{}@{}:{}/\".format(proxy_auth, proxy_host, proxy_port)}\n",
    "\n",
    "    resp1 = requests.get(url1,timeout = None, headers=headers)\n",
    "    resp2 = requests.get(url2,timeout = None, headers=headers)\n",
    "    soup1 = BeautifulSoup(resp1.text, 'html.parser')\n",
    "    soup2 = BeautifulSoup(resp2.text, 'html.parser')\n",
    "    if resp1.status_code == 200:\n",
    "        for item1 in soup1.select(\"#firstHeading\"):\n",
    "            skill_dict = {'Skill':s, 'URL':url1, 'PageHeading':item1.text}\n",
    "    else :\n",
    "        skill_dict={'Skill':s, 'URL':\"Web page does not exist\", 'PageHeading': \"NA\"}\n",
    "    if resp2.status_code == 200:\n",
    "        for item2 in soup2.select(\".mw-headline\"):\n",
    "            skill_list.append(item2.text)\n",
    "            #print(skill_list)\n",
    "        skill_dict['Other_use']= skill_list\n",
    "    else :\n",
    "        skill_dict['Other_use']= \"NA\"\n",
    "    print(i)\n",
    "    f.write(str(skill_dict)+\",\")\n",
    "        #final_data.append(skill_dict)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "p = re.compile('(?<!\\\\\\\\)\\'')\n",
    "str = p.sub('\\\"', str)\n",
    "\n",
    "with open(\"WP_scrapnew_1.json\") as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open(\"WP_scrapnew_2.json\") as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "with open(\"WP_scrapnew_3.json\") as f:\n",
    "    data3 = json.load(f)\n",
    "\n",
    "items1 = data1[\"items\"]\n",
    "#print(json.dumps(items1, indent=2))\n",
    "items2 = data2[\"items\"]\n",
    "items3 = data3[\"items\"]\n",
    "\n",
    "listitem = [items1, items2, items3]\n",
    "finaljson = {\"items\" : []}\n",
    "\n",
    "finaljson[\"items\"].append(items1)\n",
    "finaljson[\"items\"].append(items2)\n",
    "finaljson[\"items\"].append(items3)\n",
    "#print(json.dumps(finaljson, indent=2))\n",
    "\n",
    "with open(\"merged123_json.json\", \"w\") as f:\n",
    "    f.write(json.dumps(finaljson, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as df\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'\n",
    "#headers = { 'User-Agent' : user_agent }\n",
    "df = pd.read_excel(r'/Users/smani/Downloads/skills_taxonomy.xlsx')\n",
    "skills = df[\"skill\"].tolist()\n",
    "#skill_l = []\n",
    "for x in skills:\n",
    "    if isinstance(x,str):\n",
    "        for y in x :\n",
    "            if y.isalpha():\n",
    "                skill_l.append(x.lower())\n",
    "                break\n",
    "    else:\n",
    "        pass\n",
    "final_skill = list(dict.fromkeys(skill_l))\n",
    "\n",
    "with open(\"Fianl_list.json\",\"w\") as f :\n",
    "    json.dump(final_skill,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_skill.index(\"auto_dealership_service_tech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28557 - 26322\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "scrape_url = \"https://en.wikipedia.org/wiki/Transport_(disambiguation)\"\n",
    "\n",
    "res = requests.get(scrape_url)\n",
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "\n",
    "items = soup.select(\".mw-headline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related terms\n",
      "Biology and medicine\n",
      "Geology and earth science\n",
      "Physics and technology\n",
      "Other uses\n",
      "See also\n"
     ]
    }
   ],
   "source": [
    "for examples in items:\n",
    "    print (examples.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Related terms'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "scrape_url = \"https://en.wikipedia.org/wiki/Transport_(disambiguation)\"\n",
    "\n",
    "res = requests.get(scrape_url)\n",
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "\n",
    "items = soup.select(\".mw-editsection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "#import json\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore', category=GuessedAtParserWarning)\n",
    "warnings.catch_warnings()\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "desDict = dict()\n",
    "skill_error_list = []\n",
    "df = pd.read_excel(r\"/Users/smani/Downloads/UniqueCF_List.xlsx\")\n",
    "skill_list1 = df[\"Unique_CF\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.summary(\".NET Framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "#import json\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from requests.exceptions import ConnectionError\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore', category=GuessedAtParserWarning)\n",
    "warnings.catch_warnings()\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "desDict = dict()\n",
    "skill_error_list = []\n",
    "df = pd.read_excel(r\"/Users/smani/Downloads/skills_tk_batch_20201026.xls\")\n",
    "skill_list = df[\"CF\"].tolist()\n",
    "skill_list = skill_list[:1000]\n",
    "desDict = dict()\n",
    "\n",
    "def wiki_mapping(i) :\n",
    "    try :\n",
    "        desDict[\"Skill_name\"]= i\n",
    "        desDict[\"Description\"] = wikipedia.summary(i,sentences=2)\n",
    "    except wikipedia.exceptions.DisambiguationError as e :\n",
    "        skill_error_list.append(i)\n",
    "        print(e.options)\n",
    "    except wikipedia.exceptions.PageError as e :\n",
    "        skill_error_list.append(i)\n",
    "        print(e)\n",
    "    except ConnectionError as e:\n",
    "        print(e)\n",
    "    return desDict\n",
    "\n",
    "#working_list = new_skills(skill_list)\n",
    "\n",
    "p = Pool(10)\n",
    "records = p.map(wiki_mapping,skill_list)\n",
    "f1 = open(\"Desc_skills_skills_tk_batch_20201026.txt\",\"w\")\n",
    "f2 = open(\"NoDesc_skills_skills_tk_batch_20201026.txt\",\"w\")\n",
    "f1.write(str(records))\n",
    "f2.write(str(skill_error_list))\n",
    "f1.close()\n",
    "f2.close()\n",
    "#print(records)\n",
    "p.terminate()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import json\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "import time\n",
    "#warnings.filterwarnings('ignore', category=GuessedAtParserWarning)\n",
    "warnings.catch_warnings()\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "nowtime = time.time()\n",
    "#f1=open(\"Desc_Work_\" + str(nowtime) + \".json\", \"w+\")\n",
    "#f2=open(\"NoDesc_Work_\" + str(nowtime) + \".json\", \"w+\")\n",
    "desDict = dict()\n",
    "skill_error_list1 = []\n",
    "df = pd.read_excel(r\"/Users/smani/Downloads/UniqueCF_List.xlsx\")\n",
    "skill_list1 = df[\"Unique_CF\"].tolist()\n",
    "skill_list1 = skill_list1[:20]\n",
    "desDict = dict()\n",
    "\n",
    "def wiki_mapping(i) :\n",
    "    \n",
    "    try :\n",
    "        desDict[\"Skill_name\"]= i\n",
    "        desDict[\"Description\"] = wikipedia.summary(i,sentences=2)\n",
    "\n",
    "    except wikipedia.exceptions.DisambiguationError as e :\n",
    "        skill_error_list1.append(i)\n",
    "        #json.dump(skill_error_list1,f2)\n",
    "        print(e.options)\n",
    "    except wikipedia.exceptions.PageError as e :\n",
    "        skill_error_list1.append(i)\n",
    "        #json.dump(skill_error_list1, f2)\n",
    "        print(e)\n",
    "    except ConnectionError as e:\n",
    "        print(e)\n",
    "\n",
    "    return (desDict,skill_error_list1)\n",
    "\n",
    "#working_list = new_skills(skill_list)\n",
    "\n",
    "p = Pool(10)\n",
    "records = p.map(wiki_mapping,skill_list1)\n",
    "#f1.write(str(records) + \",\")\n",
    "\n",
    "#f1.close()\n",
    "#f2.close()\n",
    "#print(records)\n",
    "p.terminate()\n",
    "p.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import json\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "import time\n",
    "#warnings.filterwarnings('ignore', category=GuessedAtParserWarning)\n",
    "warnings.catch_warnings()\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "nowtime = time.time()\n",
    "#f1=open(\"FiDesc_Work_\" + str(nowtime) + \".json\", \"w+\")\n",
    "#f2=open(\"NoDesc_Work_\" + str(nowtime) + \".json\", \"w+\")\n",
    "desDict = dict()\n",
    "#skill_error_list1 = []\n",
    "df = pd.read_excel(r\"/Users/smani/Downloads/UniqueCF_List.xlsx\")\n",
    "skill_list1 = df[\"Unique_CF\"].tolist()\n",
    "skill_list1 = skill_list1[:200]\n",
    "desDict = dict()\n",
    "\n",
    "def wiki_mapping(i) :\n",
    "    desDict[\"Skill_name\"]= i\n",
    "    try :\n",
    "        \n",
    "        desDict[\"Description\"] = wikipedia.summary(i,sentences=2)\n",
    "\n",
    "    except wikipedia.exceptions.DisambiguationError as e :\n",
    "        \n",
    "        desDict[\"Description\"] = e\n",
    "        \n",
    "    except wikipedia.exceptions.PageError as e :\n",
    "        \n",
    "        desDict[\"Description\"] = e\n",
    "    \n",
    "    except ConnectionError as e:\n",
    "        pass\n",
    "        \n",
    "    return desDict\n",
    "\n",
    "#working_list = new_skills(skill_list)\n",
    "\n",
    "p = Pool(10)\n",
    "records = p.map(wiki_mapping,skill_list1)\n",
    "#f1.write(str(records) + \",\")\n",
    "\n",
    "#f1.close()\n",
    "#f2.close()\n",
    "#print(records)\n",
    "p.terminate()\n",
    "p.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "nowtime = time.time()\n",
    "\n",
    "df1 = pd.read_excel(r'/Users/smani/Downloads/esco_local.xls')\n",
    "df2 = pd.read_json(r'/Users/smani/Downloads/data.json')\n",
    "#df2 = pd.read_excel(r'/Users/smani/Downloads/2_sent.xlsx')\n",
    "\n",
    "f1=open(\"R2_Found_Desc_\" + str(nowtime) + \".json\", \"w+\")\n",
    "\n",
    "f2=open(\"R2_Found_NoDesc\" + str(nowtime) + \".json\", \"w+\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = df2[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = df2[0].tolist()\n",
    "desc_dict = dict(zip(df1.CF_2, df1.description_l))\n",
    "desc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = dict()\n",
    "res_list_nm = []\n",
    "for items in lis[:100] :\n",
    "    if items in desc_dict.keys():\n",
    "        res_dict[\"Skill_name\"]= items\n",
    "        res_dict[\"Description_2sent\"]= desc_dict[items]\n",
    "        f1.write(str(res_dict) + \",\")\n",
    "        #print(res_dict)\n",
    "    else :\n",
    "        res_list_nm.append(items)\n",
    "        #print(\"Item not found : {}\" .format(items))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "nowtime = time.time()\n",
    "\n",
    "df1 = pd.read_excel(r'/Users/smani/Downloads/Curation_Results_Local.xlsx')\n",
    "df2 = pd.read_excel(r'/Users/smani/Downloads/2_sent.xlsx')\n",
    "df3 = pd.read_excel(r'/Users/smani/Downloads/esco_local.xls')\n",
    "\n",
    "\n",
    "f1=open(\"Manual_Found_Desc_\" + str(nowtime) + \".json\", \"w+\")\n",
    "\n",
    "f2=open(\"Manual_Found_NoDesc_\" + str(nowtime) + \".json\", \"w+\")\n",
    "desc_dict1 = dict(zip(df2.Canonical, df2.Description))\n",
    "desc_dict2 = dict(zip(df3.CF_2, df3.description_l))\n",
    "\n",
    "desc_list = df1[\"Canonical Name\"].tolist()\n",
    "desc_list=list( dict.fromkeys(desc_list) ) \n",
    "res_dict = dict()\n",
    "res_list_nm = []\n",
    "for items in desc_list:\n",
    "    \n",
    "    if items in desc_dict.keys() :\n",
    "        \n",
    "        res_dict[\"Skill_name\"]= items\n",
    "        res_dict[\"Description_2sent\"]= desc_dict[items]\n",
    "        f1.write(str(res_dict) + \",\")\n",
    "        #print(res_dict)\n",
    "    else :\n",
    "        res_list_nm.append(items)\n",
    "        #print(\"Item not found : {}\" .format(items))\n",
    "        \n",
    "f2.write(str(res_list_nm) + \",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import wikipedia\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings('ignore', category=GuessedAtParserWarning)\n",
    "warnings.catch_warnings()\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "nowtime = time.time()\n",
    "\n",
    "df1 = pd.read_excel(r'/Users/smani/Downloads/Curation_Results_Local.xlsx')\n",
    "df2 = pd.read_excel(r'/Users/smani/Downloads/2_sent.xlsx')\n",
    "df3 = pd.read_excel(r'/Users/smani/Downloads/esco_local.xls')\n",
    "\n",
    "\n",
    "f1=open(\"Manual_Found_Desc_\" + str(nowtime) + \".json\", \"w+\")\n",
    "\n",
    "f2=open(\"Manual_Found_NoDesc_\" + str(nowtime) + \".json\", \"w+\")\n",
    "desc_dict1 = dict(zip(df2.Canonical, df2.Description))\n",
    "desc_dict2 = dict(zip(df3.CF_2, df3.description_l))\n",
    "\n",
    "desc_list = df1[\"Canonical Name\"].tolist()\n",
    "desc_list=list( dict.fromkeys(desc_list) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = dict()\n",
    "res_list_nm = []\n",
    "for items in desc_list:\n",
    "    res_dict[\"Skill_name\"]= items\n",
    "    \n",
    "    if items in desc_dict1.keys(): \n",
    "        res_dict[\"Description_2sent\"]= desc_dict1[items]\n",
    "        #print(res_dict)\n",
    "    \n",
    "    if items in desc_dict2.keys() :\n",
    "        res_dict[\"Description_2sent\"]= desc_dict2[items]\n",
    "        #f1.write(str(res_dict) + \",\")\n",
    "        #print(res_dict)\n",
    "    else :\n",
    "        res_list_nm.append(items)\n",
    "        #print(\"Item not found : {}\" .format(items))\n",
    "        \n",
    "wp_dict = {}\n",
    "skill_noWP = []\n",
    "\n",
    "for entry in res_list_nm :\n",
    "    try :\n",
    "        wp_dict[\"Skills_name\"]= entry\n",
    "        wp_dict[\"Description_WP\"] = wikipedia.summary(entry,sentences=2)\n",
    "    \n",
    "        #skill_dict[\"Skill_name\"]= items\n",
    "        #skill_dict[\"Description\"] = wikipedia.summary(items,sentences=2)\n",
    "        #f1.write((str(skill_dict) + \",\"))\n",
    "        print (wp_dict)\n",
    "\n",
    "    except wikipedia.exceptions.DisambiguationError as e :\n",
    "        skill_noWP.append(entry)\n",
    "        #f2.write((str(skill_no_match_list) + \",\")) \n",
    "        #json.dump(skill_error_list1,f2)\n",
    "        #print(e.options)\n",
    "    except wikipedia.exceptions.PageError as e :\n",
    "        skill_noWP.append(entry)\n",
    "        #f2.write((str(skill_no_match_list) + \",\")) \n",
    "        #json.dump(skill_error_list1, f2)\n",
    "        #print(e)\n",
    "    except ConnectionError as e:\n",
    "        pass\n",
    "        #print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_list_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import warnings\n",
    "import time\n",
    "#warnings.filterwarnings('ignore', category=GuessedAtParserWarning)\n",
    "warnings.catch_warnings()\n",
    "warnings.simplefilter(\"ignore\")\n",
    "wp_dict = {}\n",
    "skill_noWP = []\n",
    "\n",
    "for entry in res_list_nm :\n",
    "    try :\n",
    "        wp_dict[\"Skills_name\"]= entry\n",
    "        wp_dict[\"Description_WP\"] = wikipedia.summary(entry,sentences=2)\n",
    "    \n",
    "        #skill_dict[\"Skill_name\"]= items\n",
    "        #skill_dict[\"Description\"] = wikipedia.summary(items,sentences=2)\n",
    "        #f1.write((str(skill_dict) + \",\"))\n",
    "        #print (skill_dict)\n",
    "\n",
    "    except wikipedia.exceptions.DisambiguationError as e :\n",
    "        skill_noWP.append(entry)\n",
    "        #f2.write((str(skill_no_match_list) + \",\")) \n",
    "        #json.dump(skill_error_list1,f2)\n",
    "        #print(e.options)\n",
    "    except wikipedia.exceptions.PageError as e :\n",
    "        skill_noWP.append(entry)\n",
    "        #f2.write((str(skill_no_match_list) + \",\")) \n",
    "        #json.dump(skill_error_list1, f2)\n",
    "        #print(e)\n",
    "    except ConnectionError as e:\n",
    "        pass\n",
    "        #print(e)\n",
    "   \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_no_matchWP_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "nowtime = time.time()\n",
    "\n",
    "df1 = pd.read_excel(r'/Users/smani/Downloads/esco_local.xls')\n",
    "df2 = pd.read_json(r'/Users/smani/Downloads/data.json')\n",
    "\n",
    "f1=open(\"R2_Found_Desc_\" + str(nowtime) + \".json\", \"w+\")\n",
    "\n",
    "#f2=open(\"R2_Found_NoDesc\" + str(nowtime) + \".json\", \"w+\")\n",
    "\n",
    "lis = df2[0].tolist()\n",
    "desc_dict = dict(zip(df1.CF_2, df1.description_l))\n",
    "res_dict = dict()\n",
    "res_list_nm = []\n",
    "for items in lis[:200] :\n",
    "    if items in desc_dict.keys():\n",
    "        res_dict[\"Skill_name\"]= items\n",
    "        res_dict[\"Description_2sent\"]= desc_dict[items]\n",
    "        f1.write(str(res_dict) + \",\")\n",
    "        print(res_dict)\n",
    "    else :\n",
    "        res_list_nm.append(items)\n",
    "        #print(\"Item not found : {}\" .format(items))\n",
    "#f2.write(str(res_list_nm) + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.read_json(r'/Users/smani/Downloads/data.json')\n",
    "#df3 = pd.read_json(r'/Users/smani/Downloads/data-2.json')\n",
    "df4 = pd.read_json(r'/Users/smani/Downloads/data-3.json',lines = True)\n",
    "#df5 = pd.read_json(r'/Users/smani/Downloads/data-5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(r\"/Users/smani/PycharmProjects/Learning/Skill desc/V5_Found_Desc_1603986325.6425972.json\", encoding= \"utf8\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(line) for line in data] #convert string to dict format\n",
    "df = pd.read_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected character found when decoding array value (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-74761c77fb54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/Users/smani/Downloads/data-6.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1093\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             )\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected character found when decoding array value (2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df7 = pd.read_json(r'/Users/smani/Downloads/data-6.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "\n",
    "try:\n",
    "    df7 = pd.read_json(r'/Users/smani/Downloads/data-6.json')\n",
    "except JSONDecodeError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
